{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python373jvsc74a57bd07a47bf6e4dce792c65a7d49caf54e729a54726c5dc0326382809f81559c32857",
      "display_name": "Python 3.7.3 64-bit ('base': conda)"
    },
    "colab": {
      "name": "CIFAR10_CNN_Model_ColabReady.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmerFakher/INT2-Group-Project-CIFAR10/blob/Automated-Dataset-DownloadExtractPicklePlot-UF/CIFAR10_CNN_Model_ColabShortened.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNAtdyZQDovL"
      },
      "source": [
        "# Colab Ready CNN Go To https://colab.research.google.com/github/\n",
        "\n",
        "**Go to Runtime -> change runtime type and select hardware as GPU :) Super fast**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8v9mduoK037"
      },
      "source": [
        "## Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEScmBFIDovY"
      },
      "source": [
        "### We tried a Artificial Neural Network Naive Model\n",
        "\n",
        "*SEE CIFAR10_ANN_NaiveModel.ipynb*\n",
        "\n",
        "### **We get a low training accuracy and testing accuracy so we must change our approach. SEE CNN BELOW**\n",
        "\n",
        "Artificial neural network for classifying data into multiple (10) classes. \n",
        "\n",
        "Here we use Softmax. Given a specific example Softmax will output probabilities (probabilities of that example being each class) where all of them add up to 1. Note that here we are assuming that each example belongs to exactly one class and can't have multiple labels (e.g. Dog and Frog in this case wouldn't make sense but for classifying the genre of different Netflix movies you may have one movie that is Action, Sci-Fi, and comedy). We would also have to use a different function other than softmax and different approach.\n",
        "\n",
        "Although our images are fairly complicated. This might have been fine for numerical data and classifying that or simple images e.g. black and white letters or something.\n",
        "\n",
        "* Here this is going to require too much computation \n",
        "* It also is at risk of overfitting if there is too much variation and we can see that some of these cats for example, are in different positions and parts of the image and our network will struggle with this\n",
        "* As we are feeding in data flattened (each image is just an array of numeric values 1x 3072 vector) it literally treats all of the parts of the image the same and data which is close together (in 2D maybe they are together) the same as far apart data \n",
        "\n",
        "Our images in 2D would be 32 x 32 x 3 size. 32 by 32 for dimensions height and width and the 3 for the colour depth (rgb, red green blue values like 120, -63, 24). \n",
        "\n",
        "Flattened each image is 1 x 3072. \n",
        "\n",
        "In the dataset folder \"cifar-10-batches-py\":\n",
        "there are data batch 1 to 5 and a test batch\n",
        "\n",
        "* data batch 1 to 5 are used for training and in total therefore there are 50000 images used for training\n",
        "* test batch has 10000 image\n",
        "\n",
        "Each batch has 10000 images so e.g. data batch 1 for training has 10000 images\n",
        "\n",
        "As a flattened image is 1 x 3072 vector the whole data matrix for data batch 1 (used for training) is (10000, 3072)\n",
        "We train this network on all 5 batches. Each batch it sees 10 times (thats why its 10 epochs and it starts at like low accuracy and works its way up). Then we test the network on the 1 testing batch using this unseen data to see if it can actually \"see\" and classify images correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2eTQ2yBK-Nm"
      },
      "source": [
        "# **CIFAR10 Convolutional Neural Network Classifier**\n",
        "\n",
        "This is better suited so we get higher accuracy on this complex image dataset than the flawed naive network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep6FiPakKv2F"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z72yl2A_DovZ"
      },
      "source": [
        "# Using our working Data Download and Loading Modules\n",
        "# import File_Manager_Tar_Gz as fm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN7AIsCfDova"
      },
      "source": [
        "# For plotting and data pre-processing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1XxUPVfDova"
      },
      "source": [
        "# Machine Learning Module Used for Neural Network Classification\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snl6vaEvDova"
      },
      "source": [
        "## Data Download and Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB-Y1LR7Dova"
      },
      "source": [
        "# def download_and_extract_data():\n",
        "#     file_name = \"cifar-10-python.tar.gz\"\n",
        "#     website_link = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "\n",
        "#     print(fm.download_file_tar_gz(dataset_link=website_link, dataset_file_name=file_name))\n",
        "#     print(fm.open_file_tar_gz(file_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXbV5RfRDovb"
      },
      "source": [
        "# download_and_extract_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rwzgSUDDovb"
      },
      "source": [
        "### Store all batches in a dictionary called 'data'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvpseS5WDovb"
      },
      "source": [
        "# def load_data(file_names : list, folder_names : list):\n",
        "#     data_dico = {}\n",
        "#     for fil, fold in zip(file_names, folder_names):\n",
        "#         batch_file_path = \"{}/{}\".format(fold, fil)\n",
        "#         dico = fm.unpickle(batch_file_path)\n",
        "#         data_dico[dico] = dico\n",
        "\n",
        "#     return data_dico"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CGgWgLcDovc"
      },
      "source": [
        "# file_names = [\"data_batch_1\", \"data_batch_2\", \"data_batch_3\", \"data_batch_4\", \"data_batch_5\", \"test_batch\"]\n",
        "# data = load_data(file_names, [\"cifar-10-batches-py\"]*len(file_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HkYMB7XKkLX"
      },
      "source": [
        "## Cifar data get from tf data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxJpfsP6Dovc"
      },
      "source": [
        "def tf_data_get_cifar():\n",
        "    from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "    (train_X, train_y), (test_X, test_y) = cifar10.load_data()\n",
        "    # print(train_X.shape, test_X.shape)\n",
        "    # train_X is (50000, 32, 32, 3), test_X is (10000, 32, 32, 3)\n",
        "    return (train_X, train_y), (test_X, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BfcAozdJaPG"
      },
      "source": [
        "## Other data dico load etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rm9kCPNDovc"
      },
      "source": [
        "# arr = np.array([1,2,3,4,5,6,7,8,9,10,11])\n",
        "# np.array_split(arr, 3) # [array([1, 2, 3, 4]), array([5, 6, 7, 8]), array([ 9, 10, 11])]\n",
        "\n",
        "# arr = np.array_split(train_X, 5)\n",
        "# print(len(arr))\n",
        "# for _ in arr:\n",
        "#     print(_.shape)\n",
        "# 5\n",
        "# (10000, 32, 32, 3)\n",
        "# (10000, 32, 32, 3)\n",
        "# (10000, 32, 32, 3)\n",
        "# (10000, 32, 32, 3)\n",
        "# (10000, 32, 32, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mTGfJ3PDovd",
        "outputId": "4997279a-4396-420c-c48f-fecb61a05240"
      },
      "source": [
        "\"\"\"\"\"\n",
        "def tf_data_build_batches_dico(name, xs, ys):\n",
        "    # Use temp_dic to build this\n",
        "    # data[b'training batch 1 of 5'].keys()\n",
        "    # >>> dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
        "\n",
        "    temp_dic = {}\n",
        "    temp_dic[b'batch_label'] = name\n",
        "    temp_dic[b'labels'] = ys\n",
        "    temp_dic[b'data'] = xs\n",
        "    # No b'filenames' made as not part of cifar from tf data\n",
        "\n",
        "    return temp_dic\n",
        "\n",
        "def tf_data_to_batches_dico_format():\n",
        "    (train_X, train_y), (test_X, test_y) = tf_data_get_cifar()\n",
        "\n",
        "    # data.keys()\n",
        "    # >>> dict_keys([b'training batch 1 of 5', b'training batch 2 of 5', b'training batch 3 of 5', \n",
        "    #                    b'training batch 4 of 5', b'training batch 5 of 5', b'testing batch 1 of 1'])\n",
        "    data_dico = {}\n",
        "    tr_batch_names = [b'training batch 1 of 5', b'training batch 2 of 5', b'training batch 3 of 5', \n",
        "                    b'training batch 4 of 5', b'training batch 5 of 5']\n",
        "    \n",
        "    # Add Training data batches into data_dico\n",
        "    train_X_split = np.array_split(train_X, 5)  # split numpy array into a list of 5 numpy arrays\n",
        "    train_y_split = np.array_split(train_y, 5)\n",
        "\n",
        "    for i in range(len(tr_batch_names)): # put all batche dicos into master data_dico according to batch name\n",
        "        # No b'filenames' used\n",
        "        data_dico[tr_batch_names[i]] = tf_data_build_batches_dico(tr_batch_names[i], train_X_split[i], train_y_split[i])\n",
        "\n",
        "    # Add tesing data batche into data_dico\n",
        "    data_dico[b'testing batch 1 of 1'] = tf_data_build_batches_dico(b'testing batch 1 of 1', test_X, test_y)\n",
        "\n",
        "    return data_dico\n",
        "\n",
        "data = tf_data_to_batches_dico_format()\n",
        "\"\"\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"\"\\ndef tf_data_build_batches_dico(name, xs, ys):\\n    # Use temp_dic to build this\\n    # data[b\\'training batch 1 of 5\\'].keys()\\n    # >>> dict_keys([b\\'batch_label\\', b\\'labels\\', b\\'data\\', b\\'filenames\\'])\\n\\n    temp_dic = {}\\n    temp_dic[b\\'batch_label\\'] = name\\n    temp_dic[b\\'labels\\'] = ys\\n    temp_dic[b\\'data\\'] = xs\\n    # No b\\'filenames\\' made as not part of cifar from tf data\\n\\n    return temp_dic\\n\\ndef tf_data_to_batches_dico_format():\\n    (train_X, train_y), (test_X, test_y) = tf_data_get_cifar()\\n\\n    # data.keys()\\n    # >>> dict_keys([b\\'training batch 1 of 5\\', b\\'training batch 2 of 5\\', b\\'training batch 3 of 5\\', \\n    #                    b\\'training batch 4 of 5\\', b\\'training batch 5 of 5\\', b\\'testing batch 1 of 1\\'])\\n    data_dico = {}\\n    tr_batch_names = [b\\'training batch 1 of 5\\', b\\'training batch 2 of 5\\', b\\'training batch 3 of 5\\', \\n                    b\\'training batch 4 of 5\\', b\\'training batch 5 of 5\\']\\n    \\n    # Add Training data batches into data_dico\\n    train_X_split = np.array_split(train_X, 5)  # split numpy array into a list of 5 numpy arrays\\n    train_y_split = np.array_split(train_y, 5)\\n\\n    for i in range(len(tr_batch_names)): # put all batche dicos into master data_dico according to batch name\\n        # No b\\'filenames\\' used\\n        data_dico[tr_batch_names[i]] = tf_data_build_batches_dico(tr_batch_names[i], train_X_split[i], train_y_split[i])\\n\\n    # Add tesing data batche into data_dico\\n    data_dico[b\\'testing batch 1 of 1\\'] = tf_data_build_batches_dico(b\\'testing batch 1 of 1\\', test_X, test_y)\\n\\n    return data_dico\\n\\ndata = tf_data_to_batches_dico_format()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPr3ZNTsDove",
        "outputId": "e3cf0c25-ff13-4d56-a1a5-d5ed11c9d3af"
      },
      "source": [
        "\"\"\"\"\"\n",
        "for k in data.keys():\n",
        "    print(data[k][b'batch_label'])\n",
        "    print(data[k][b'labels'].shape)\n",
        "    print(data[k][b'data'].shape)\n",
        "\"\"\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"\"\\nfor k in data.keys():\\n    print(data[k][b\\'batch_label\\'])\\n    print(data[k][b\\'labels\\'].shape)\\n    print(data[k][b\\'data\\'].shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ENRwOGdDovf"
      },
      "source": [
        "# data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9YiLfTfDovf"
      },
      "source": [
        "# data[b'training batch 1 of 5'].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MgcqhuFDovf"
      },
      "source": [
        "# data[b'training batch 1 of 5'][b'batch_label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NYnjjWVM1_A"
      },
      "source": [
        "## label classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5lnms08Dovf"
      },
      "source": [
        "# From cifar-10 https://www.cs.toronto.edu/~kriz/cifar.html our 10 classes\n",
        "label_classes = [\"airplane\", \"automobile\", \"bird\", \"cat\",\t\"deer\",\t\"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCbT8O_7MxIY"
      },
      "source": [
        "## further dico load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkW3Ya8wDovg"
      },
      "source": [
        "```python\n",
        "data.keys()\n",
        ">>> dict_keys([b'training batch 1 of 5', b'training batch 2 of 5', b'training batch 3 of 5', b'training batch 4 of 5', b'training batch 5 of 5', b'testing batch 1 of 1'])\n",
        "\n",
        "\n",
        "data[b'training batch 1 of 5'] # >>>> png names.....\n",
        "\n",
        "data[b'training batch 1 of 5'].keys() \n",
        ">>> dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
        "\n",
        "batch 1 training data and labels\n",
        "print(data[b'training batch 1 of 5'][b'data'].shape)     # >>> (10000, 3072)     \n",
        "print(len(data[b'training batch 1 of 5'][b'labels']))    # >>> 10000\n",
        "\n",
        "data.keys() \n",
        ">>> dict_keys([b'training batch 1 of 5', b'training batch 2 of 5', b'training batch 3 of 5', b'training batch 4 of 5', b'training batch 5 of 5', b'testing batch 1 of 1'])\n",
        "\n",
        "data[b'training batch 1 of 5'] # >>>> png names.....\n",
        "\n",
        "data[b'training batch 1 of 5'].keys() \n",
        ">>> dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
        "\n",
        "batch 1 training data and labels\n",
        "print(data[b'training batch 1 of 5'][b'data'].shape)     # >>> (10000, 3072)     \n",
        "print(len(data[b'training batch 1 of 5'][b'labels']))    # >>> 10000\n",
        "\n",
        "\n",
        "(10000, 3072)\n",
        "10000 labels as it is batch 1 of 5 ie 10000 out of 50000 images\n",
        "32 x 32 pixels image # 3 colour depth 32323 = 3072\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3NOrBQ-Dovg"
      },
      "source": [
        "## Explain Data Pre-processing\n",
        "\n",
        "We may refer to data as data_dico to make it clear these are just example code so they are not accidently run without proper intention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p23zTR3-Jpwy"
      },
      "source": [
        "### Data Pre-processing Note"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBjN6bmFDovg"
      },
      "source": [
        "```python\n",
        "# Looking at the training batch 1 training examples\n",
        "\n",
        "data_dico[b'training batch 1 of 5'][b'data'].shape # stored 10000 flattened images in 1d arrays each of size 3072\n",
        ">>> (10000, 3072)\n",
        "\n",
        "data_dico[b'training batch 1 of 5'][b'data'][0] # The 1st image which is stored as a row of (3072,)\n",
        ">>> array([ 59,  43,  50, ..., 140,  84,  72], dtype=uint8)\n",
        "```\n",
        "\n",
        "According to [cifar-10 dataset website](https://www.cs.toronto.edu/~kriz/cifar.html), it confirms that yes this is a 10000 by 3072 2D numpy array and that each of these rows (which are numpy arrays 1 x 3072 aka (3072,)) represent a 32 by 32 coloured image.\n",
        "\n",
        "Each image is stored as RGB values. It also tells us that the first 1024 values in this row represent the red colour values. The next 1024 values are green with the last 1024 being blue. So in total that is 3072 data values for each image.\n",
        "\n",
        "So if we wanted to visualise an image from this array 3072 of data points in 2 dimensions so it is a 32 by 32 image by 3 for 3 colour depth, then we should note that the image is in 'row-major order' so the first 32 values of the array would be red colour values for the first row of the image.\n",
        "\n",
        "Therefore we must reshape this data to 32 by 32 by 3 (32 rows, 32 columns, 3 colour depth) format so we can plot it in matplotlib for visualisation.\n",
        "\n",
        "So first due to the 'row-major' order as well we must do 2 operations;\n",
        "* The first will reshape each 3072 array (from our 10000 by 3072 matrix), into a 3 by 32 by 32 3D numpy array\n",
        "    * with the 1st dimension representing the 3 RGB channels\n",
        "    * with the 2nd dimension representing the 32 pixels in a row\n",
        "    * and finally the 3rd dimension representing the 32 columns\n",
        "\n",
        "* The second operation will just transpose our matrix so we can put that 1st RGB dimension at the back, so we turn the shape from (3,32,32) to a (32,32,3)\n",
        "    * we do this by passing in parameters into the transpose function which labels the columns [RGB, pixel rows,  pixel columns] as [1,2,0] and re-orders       them as [0,1,2] so [pixel rows,  pixel columns, RGB]. Therefore we can put that 1st RGB dimension at the back. This means each RGB value for each pixel (of each image) will be stored together as shown below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehNE7dwvJ2_A"
      },
      "source": [
        "### Reshape and Transpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5-lMLihDovh"
      },
      "source": [
        "Example on just 1 image:\n",
        "```python\n",
        "\n",
        "an_image = data_dico[b'training batch 1 of 5'][b'data'][0].reshape([3,32,32]).transpose([1,2,0])\n",
        "print(an_image[0][0].shape)\n",
        ">>> (3,)\n",
        "print(an_image[0][0]) # a pixel 3 with r g b values \n",
        ">>> [59, 62, 63]\n",
        "\n",
        "# Reshape (3,32,32) as \n",
        "# 3 is for r g b values\n",
        "# 32 is for row of pixels\n",
        "# other 32 is for column of pixels\n",
        "\n",
        "# Transpose to (32,32,3) to store RBG values together for each pixel for each image\n",
        "# 32 is for row of pixels\n",
        "# other 32 is for column of pixels\n",
        "# 3 is for r g b values\n",
        "\n",
        "We convert from row-major or column-major order\n",
        "```\n",
        "#### We can execute this reshape and transpose on the whole dataset in this next normalise function but we will just see how it effects images individually when we plot them below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWX-bGYoDovi"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIQpmzvcJ83x"
      },
      "source": [
        "### RGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b5Vxhr1Dovi"
      },
      "source": [
        "# data[b'training batch 1 of 5'][b'data'][0].T[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0ete99eDovi",
        "outputId": "d23364ad-d256-492e-cf1e-d283b177d0d8"
      },
      "source": [
        "\"\"\"\"\"\n",
        "def image_RGB_channels_viewer(data_dico, batch_name, index):\n",
        "    if True: #if data[batch_name][b'data'][index].ndim == 1: # check if 1 flattened vector for image\n",
        "        fig, ax = plt.subplots(1,3)\n",
        "        # ax[0].imshow(data_dico[batch_name][b'data'][index].reshape([3,32,32])[0], cmap='Reds_r')\n",
        "        # ax[1].imshow(data_dico[batch_name][b'data'][index].reshape([3,32,32])[1], cmap='Greens_r')\n",
        "        # ax[2].imshow(data_dico[batch_name][b'data'][index].reshape([3,32,32])[2], cmap='Blues_r')\n",
        "        ax[0].imshow(data_dico[batch_name][b'data'][index].T[0], cmap='Reds_r')\n",
        "        ax[1].imshow(data_dico[batch_name][b'data'][index].T[1], cmap='Greens_r')\n",
        "        ax[2].imshow(data_dico[batch_name][b'data'][index].T[2], cmap='Blues_r')\n",
        "\n",
        "        # FOR CIFAR from tf data\n",
        "        l = \"\"\n",
        "        try:\n",
        "            l = label_classes[data_dico[batch_name][b'labels'][index]] # if labels stored as list not from tf data\n",
        "        except:\n",
        "            pass\n",
        "        try: \n",
        "            # 0 is for numpy array in case array([6], dtype=uint8) is returned\n",
        "            l = label_classes[data_dico[batch_name][b'labels'][index][0]] # if labels are already numpy array\n",
        "        except:\n",
        "            pass\n",
        "        # FOR CIFAR from tf data\n",
        "\n",
        "        ax[1].set_title(\"3 RGB channels for 1 image\\n\"+l, size=25)\n",
        "        plt.show()\n",
        "    # else:\n",
        "    #     print(\"ERROR: Data provided is not 1 flattened vector for image. Please use plot_image_data instead. Data dimension currently is: {}\".format(data_dico[batch_name][b'data'][index].ndim))\n",
        "\n",
        "# Plot 3 RGB channels for training image in batch 1 separately \n",
        "image_RGB_channels_viewer(data, b'training batch 1 of 5', 6)\n",
        "\"\"\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"\"\\ndef image_RGB_channels_viewer(data_dico, batch_name, index):\\n    if True: #if data[batch_name][b\\'data\\'][index].ndim == 1: # check if 1 flattened vector for image\\n        fig, ax = plt.subplots(1,3)\\n        # ax[0].imshow(data_dico[batch_name][b\\'data\\'][index].reshape([3,32,32])[0], cmap=\\'Reds_r\\')\\n        # ax[1].imshow(data_dico[batch_name][b\\'data\\'][index].reshape([3,32,32])[1], cmap=\\'Greens_r\\')\\n        # ax[2].imshow(data_dico[batch_name][b\\'data\\'][index].reshape([3,32,32])[2], cmap=\\'Blues_r\\')\\n        ax[0].imshow(data_dico[batch_name][b\\'data\\'][index].T[0], cmap=\\'Reds_r\\')\\n        ax[1].imshow(data_dico[batch_name][b\\'data\\'][index].T[1], cmap=\\'Greens_r\\')\\n        ax[2].imshow(data_dico[batch_name][b\\'data\\'][index].T[2], cmap=\\'Blues_r\\')\\n\\n        # FOR CIFAR from tf data\\n        l = \"\"\\n        try:\\n            l = label_classes[data_dico[batch_name][b\\'labels\\'][index]] # if labels stored as list not from tf data\\n        except:\\n            pass\\n        try: \\n            # 0 is for numpy array in case array([6], dtype=uint8) is returned\\n            l = label_classes[data_dico[batch_name][b\\'labels\\'][index][0]] # if labels are already numpy array\\n        except:\\n            pass\\n        # FOR CIFAR from tf data\\n\\n        ax[1].set_title(\"3 RGB channels for 1 image\\n\"+l, size=25)\\n        plt.show()\\n    # else:\\n    #     print(\"ERROR: Data provided is not 1 flattened vector for image. Please use plot_image_data instead. Data dimension currently is: {}\".format(data_dico[batch_name][b\\'data\\'][index].ndim))\\n\\n# Plot 3 RGB channels for training image in batch 1 separately \\nimage_RGB_channels_viewer(data, b\\'training batch 1 of 5\\', 6)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a1RM1CKDp5"
      },
      "source": [
        "### plot image data function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWC8Iol6KCD1"
      },
      "source": [
        "def plot_image_data(the_image_data, label):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(the_image_data)\n",
        "    ax.set_title(label, size=25)\n",
        "    return fig, ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTLCZqhcJ_oM"
      },
      "source": [
        "### plotting images using dico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shw5NGf2Dovj",
        "outputId": "4fc03542-e5f5-4f02-909a-0bb61a24d9f6"
      },
      "source": [
        "\"\"\"\"\"\n",
        "def image_viewer_from_flattened_data(data_dico, batch_name, index):\n",
        "    if True: #data_dico[batch_name][b'data'][index].ndim == 1: # check if 1 flattened vector for image\n",
        "        \n",
        "        # FOR CIFAR from tf data\n",
        "        l = \"\"\n",
        "        try:\n",
        "            l = label_classes[data_dico[batch_name][b'labels'][index]] # if labels stored as list not from tf data\n",
        "        except:\n",
        "            pass\n",
        "        try: \n",
        "            # 0 is for numpy array in case array([6], dtype=uint8) is returned\n",
        "            l = label_classes[data_dico[batch_name][b'labels'][index][0]] # if labels are already numpy array\n",
        "        except:\n",
        "            pass\n",
        "        # FOR CIFAR from tf data\n",
        "\n",
        "        # a_fig, a_ax = plot_image_data(the_image_data=data_dico[batch_name][b'data'][index].reshape([3,32,32]).transpose([1,2,0]),label=l)\n",
        "        a_fig, a_ax = plot_image_data(the_image_data=data_dico[batch_name][b'data'][index],label=l)\n",
        "        plt.show()\n",
        "    # else:\n",
        "    #     print(\"ERROR: Data provided is not 1 flattened vector for image. Please use plot_image_data instead. Data dimension currently is: {}\".format(data_dico[batch_name][b'data'][index].ndim))\n",
        "\n",
        "# training image in batch 1\n",
        "# 3D Numpy array Real image. Default settings from imshow.\n",
        "image_viewer_from_flattened_data(data, batch_name=b'training batch 1 of 5', index=6)\n",
        "\"\"\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"\"\\ndef image_viewer_from_flattened_data(data_dico, batch_name, index):\\n    if True: #data_dico[batch_name][b\\'data\\'][index].ndim == 1: # check if 1 flattened vector for image\\n        \\n        # FOR CIFAR from tf data\\n        l = \"\"\\n        try:\\n            l = label_classes[data_dico[batch_name][b\\'labels\\'][index]] # if labels stored as list not from tf data\\n        except:\\n            pass\\n        try: \\n            # 0 is for numpy array in case array([6], dtype=uint8) is returned\\n            l = label_classes[data_dico[batch_name][b\\'labels\\'][index][0]] # if labels are already numpy array\\n        except:\\n            pass\\n        # FOR CIFAR from tf data\\n\\n        # a_fig, a_ax = plot_image_data(the_image_data=data_dico[batch_name][b\\'data\\'][index].reshape([3,32,32]).transpose([1,2,0]),label=l)\\n        a_fig, a_ax = plot_image_data(the_image_data=data_dico[batch_name][b\\'data\\'][index],label=l)\\n        plt.show()\\n    # else:\\n    #     print(\"ERROR: Data provided is not 1 flattened vector for image. Please use plot_image_data instead. Data dimension currently is: {}\".format(data_dico[batch_name][b\\'data\\'][index].ndim))\\n\\n# training image in batch 1\\n# 3D Numpy array Real image. Default settings from imshow.\\nimage_viewer_from_flattened_data(data, batch_name=b\\'training batch 1 of 5\\', index=6)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkj5qvo-Dovj"
      },
      "source": [
        "# Testing image in testing batch\n",
        "# image_viewer_from_flattened_data(data, batch_name=b'testing batch 1 of 1', index=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs74MNrvDovk"
      },
      "source": [
        "## Execute Data Pre-processing: Normalisation and Reshape Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu3oTIwaDovk"
      },
      "source": [
        "### Quick Check before Normalisation\n",
        "\n",
        "```Python\n",
        "print(\"Training set min: {}, max: {}\".format(b1_dico[b'data'].min(), b1_dico[b'data'].max())) # Training set min: 0, max: 255\n",
        "print(\"Labels min: {}, max: {}\".format(min(b1_dico[b'labels']), max(b1_dico[b'labels']))) # Labels min: 0, max: 9\n",
        "```\n",
        "Data values will be between 0 and 255 and we will check this during the normalising process for each batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht7ejCiWDovk"
      },
      "source": [
        "We see that a singular image can be changed using: \n",
        "```python\n",
        "data_dico[b'training batch 1 of 5'][b'data'][index].reshape([3,32,32]).transpose([1,2,0])\n",
        "```\n",
        "For a whole batch of images we can do this: \n",
        "```python\n",
        "data_dico[b'training batch 1 of 5'][b'data'].shape\n",
        "# >>> (10000, 3072)\n",
        "data_dico[b'training batch 1 of 5'][b'data'].reshape(10000, 3, 32, 32).transpose([0,2,3,1]).shape  # convert from 2D numpy array (10000,3072) to 4D numpy array (10000, 32, 32, 3)\n",
        "# >>> (10000, 32, 32, 3)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bMl_JEGDovk",
        "outputId": "42a5c11d-6aa7-4a36-dd86-a10b1f6d49eb"
      },
      "source": [
        "'''''\n",
        "def normalise_data(data_dico):\n",
        "    \"\"\" For each batch data in data dico, normalise this data between 0 and 1 and convert reshape data matrices for proper image representation.\n",
        "\n",
        "    E.g. Convert a batch from convert from 2D numpy array (10000,3072) to 4D numpy array (10000, 32, 32, 3)\n",
        "    where 10000 is number of training/testing examples, 32 pixel row, 32 pixel column, 3 colour depth.\n",
        "    As a result, each image/example in the 4D numpy array of a batch, is now represented as a 3D numpy array (32, 32, 3) rather than rather than a 1D numpy array of size (3072,).\n",
        "    \n",
        "    :param data_dico: dictionary of format described above.\n",
        "    :return: Changes data dictionary passed and returns\n",
        "    \"\"\"\n",
        "\n",
        "    for key in data_dico.keys(): # A key represent a name of batch. Includes training batch 1 to 5 and testing batch.\n",
        "    \n",
        "        # Normalise data from 0 and 255 to between 0 and 1    \n",
        "        data_dico[key][b'data'] = data_dico[key][b'data'] / data_dico[key][b'data'].max()\n",
        "\n",
        "        # Convert a batch from convert from 2D numpy array (10000,3072) to 4D numpy array (10000, 32, 32, 3) although 10000 can be any number of examples:\n",
        "        #\n",
        "        #DONT need for CIFAR10 from tf data\n",
        "        #\n",
        "        #n = data_dico[key][b'data'].shape[0] # number of training/testing examples\n",
        "        #data_dico[key][b'data'] = data_dico[key][b'data'].reshape(n,3,32,32).transpose([0,2,3,1])\n",
        "\n",
        "    return data_dico\n",
        "'''''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\'\\'\\ndef normalise_data(data_dico):\\n    \"\"\" For each batch data in data dico, normalise this data between 0 and 1 and convert reshape data matrices for proper image representation.\\n\\n    E.g. Convert a batch from convert from 2D numpy array (10000,3072) to 4D numpy array (10000, 32, 32, 3)\\n    where 10000 is number of training/testing examples, 32 pixel row, 32 pixel column, 3 colour depth.\\n    As a result, each image/example in the 4D numpy array of a batch, is now represented as a 3D numpy array (32, 32, 3) rather than rather than a 1D numpy array of size (3072,).\\n    \\n    :param data_dico: dictionary of format described above.\\n    :return: Changes data dictionary passed and returns\\n    \"\"\"\\n\\n    for key in data_dico.keys(): # A key represent a name of batch. Includes training batch 1 to 5 and testing batch.\\n    \\n        # Normalise data from 0 and 255 to between 0 and 1    \\n        data_dico[key][b\\'data\\'] = data_dico[key][b\\'data\\'] / data_dico[key][b\\'data\\'].max()\\n\\n        # Convert a batch from convert from 2D numpy array (10000,3072) to 4D numpy array (10000, 32, 32, 3) although 10000 can be any number of examples:\\n        #\\n        #DONT need for CIFAR10 from tf data\\n        #\\n        #n = data_dico[key][b\\'data\\'].shape[0] # number of training/testing examples\\n        #data_dico[key][b\\'data\\'] = data_dico[key][b\\'data\\'].reshape(n,3,32,32).transpose([0,2,3,1])\\n\\n    return data_dico\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luke-tFwDovl"
      },
      "source": [
        "# data = normalise_data(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wI0Y8awDovl"
      },
      "source": [
        "### Final Check to see if normalisation and reshaping done correctly by plotting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFFd0sg7Dovl",
        "outputId": "cc04b2c7-9da3-4943-872d-554ae430a190"
      },
      "source": [
        "\"\"\"\"\"\n",
        "def image_viewer_from_4D_data(dico, batch_name, index):\n",
        "    # FOR CIFAR from tf data\n",
        "    l = \"\"\n",
        "    try:\n",
        "        l = label_classes[data_dico[batch_name][b'labels'][index]] # if labels stored as list not from tf data\n",
        "    except:\n",
        "        pass\n",
        "    try: \n",
        "        # 0 is for numpy array in case array([6], dtype=uint8) is returned\n",
        "        l = label_classes[data_dico[batch_name][b'labels'][index][0]] # if labels are already numpy array\n",
        "    except:\n",
        "        pass\n",
        "    # FOR CIFAR from tf data\n",
        "        \n",
        "    plot_image_data(dico[batch_name][b'data'][index], l)\n",
        "    plt.show()\n",
        "\n",
        "image_viewer_from_4D_data(data, b'training batch 1 of 5', 0)\n",
        "data[b'training batch 1 of 5'][b'data'][0].shape\n",
        "\"\"\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"\"\\ndef image_viewer_from_4D_data(dico, batch_name, index):\\n    # FOR CIFAR from tf data\\n    l = \"\"\\n    try:\\n        l = label_classes[data_dico[batch_name][b\\'labels\\'][index]] # if labels stored as list not from tf data\\n    except:\\n        pass\\n    try: \\n        # 0 is for numpy array in case array([6], dtype=uint8) is returned\\n        l = label_classes[data_dico[batch_name][b\\'labels\\'][index][0]] # if labels are already numpy array\\n    except:\\n        pass\\n    # FOR CIFAR from tf data\\n        \\n    plot_image_data(dico[batch_name][b\\'data\\'][index], l)\\n    plt.show()\\n\\nimage_viewer_from_4D_data(data, b\\'training batch 1 of 5\\', 0)\\ndata[b\\'training batch 1 of 5\\'][b\\'data\\'][0].shape\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvKNLURIDovm"
      },
      "source": [
        "# data[b'training batch 1 of 5'][b'data'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8MGSOYNJDUI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ-qMx-EKXRL"
      },
      "source": [
        "## Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mG4bnvUKdKX"
      },
      "source": [
        "(train_X, train_y), (test_X, test_y) = tf_data_get_cifar()\n",
        "\n",
        "def normalise_data(example_data):\n",
        "    return example_data / example_data.max()\n",
        "\n",
        "train_X = normalise_data(train_X)\n",
        "test_X = normalise_data(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKfCzq1VMfxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "15320183-c7fa-4220-d140-dda4b87cad33"
      },
      "source": [
        "idx = 2\n",
        "plot_image_data(train_X[idx], label_classes[train_y[idx][0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Figure size 432x288 with 1 Axes>,\n",
              " <matplotlib.axes._subplots.AxesSubplot at 0x7f8398388d50>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRjk9mn7Dovm"
      },
      "source": [
        "## Neural Network Classification\n",
        "TensorFlow Machine Learning Module Used for Neural Network Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFzH2tq3Dovm",
        "outputId": "532923c9-f2ee-4cf4-f47a-1cae5360ac2f"
      },
      "source": [
        "# We use softmax for classification at the end for the 10 classes to get relative probabilities so if you add them all up you get 1\n",
        "# Highest probability will be what the network has predicted is the label for that image\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Flatten(input_shape=(32,32,3)), # Here we flatten each image back to 1D again just for experimentation with this Naive network\n",
        "#     tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "#     tf.keras.layers.Dense(len(label_classes), activation=tf.nn.softmax) # len(label_classes) is 10\n",
        "# ])\n",
        "# model.summary()\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input((32,32,3)), # 2D Image\n",
        "\n",
        "    # Feature Extraction Convolution & Pooling layers\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation = tf.nn.relu), #n filters, kernel_size is size of filter to detect specfic features\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation = tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation = tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    # Classification Layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(len(label_classes), activation=tf.nn.softmax) # len(label_classes) is 10\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                32832     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 126,730\n",
            "Trainable params: 126,730\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9p-b3D5Dovm"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# We are going to use sparse_categorical_crossentropy\n",
        "# After some testing I realise that categorical_crossentropy is used for labels being one hot encoded values for example [[0,1], [1,0]] like this \n",
        "# whereas sparse_categorical_crossentropy is for if your labels being direct values e.g. [2, 1] and this would be more efficient\n",
        "# in terms of space and times complexity as we don't need to store another list/vector for each label, we store a singular value\n",
        "#  and also it takes less time as we directly access the label from a single 1d list rather than searching through a list inside of another list to find where the 1 is that signifies the 1 hot encoded label class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ZgHrL4Dovn",
        "outputId": "b798ae1e-5932-4c7c-c8e8-05e5290c3c09"
      },
      "source": [
        "def concat_numpy_arr(dico, batch_key_info):\n",
        "    \"\"\" Used to concatenate all batches concatenated together.\n",
        "    \n",
        "    Can be used for data or labels.\n",
        "    \"\"\"\n",
        "    arr = None\n",
        "    for k in data.keys(): # k is batch name\n",
        "        if k != b'testing batch 1 of 1':\n",
        "            if arr is None:\n",
        "                arr = dico[k][batch_key_info]\n",
        "            else:\n",
        "                arr = np.concatenate((arr, dico[k][batch_key_info]))\n",
        "    return arr\n",
        "\n",
        "big_x_Train = concat_numpy_arr(data, b'data')\n",
        "big_y_Train = concat_numpy_arr(data, b'labels')\n",
        "print(big_x_Train.shape)\n",
        "print(big_x_Train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHagAX9UDovn",
        "outputId": "958decec-0053-4e7f-e77b-a0cc6f604f29"
      },
      "source": [
        "# Training on all training batches concatenated together\n",
        "model.fit(big_x_Train, big_y_Train, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.7744 - accuracy: 0.3432\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1924 - accuracy: 0.5778\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9981 - accuracy: 0.6482\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8796 - accuracy: 0.6970\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7988 - accuracy: 0.7202\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7204 - accuracy: 0.7481\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6685 - accuracy: 0.7663\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6131 - accuracy: 0.7844\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5674 - accuracy: 0.7995\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5240 - accuracy: 0.8154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f83982e5b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLZo3ZTrDovn"
      },
      "source": [
        "# for k in data.keys():\n",
        "#     if k != b'testing batch 1 of 1':\n",
        "#         # k is batch name\n",
        "#         x = data[k][b'data']\n",
        "#         y = np.array(data[k][b'labels'])\n",
        "\n",
        "#         model.fit(x, y, epochs=10) # had to but labels into a numpy array rather than a list\n",
        "\n",
        "# i = 0\n",
        "# k = list(data.keys())[i]\n",
        "# x = data[k][b'data']\n",
        "# y = np.array(data[k][b'labels'])\n",
        "# model.fit(x, y, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tVsyJX1Dovo"
      },
      "source": [
        "# i = 1\n",
        "# k = list(data.keys())[i]\n",
        "# x = data[k][b'data']\n",
        "# y = np.array(data[k][b'labels'])\n",
        "# model.fit(x, y, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO_Cy48CDovo"
      },
      "source": [
        "# i = 2\n",
        "# k = list(data.keys())[i]\n",
        "# x = data[k][b'data']\n",
        "# y = np.array(data[k][b'labels'])\n",
        "# model.fit(x, y, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejTL0udmDovo"
      },
      "source": [
        "# i = 3\n",
        "# k = list(data.keys())[i]\n",
        "# x = data[k][b'data']\n",
        "# y = np.array(data[k][b'labels'])\n",
        "# model.fit(x, y, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU05-PZgDovo"
      },
      "source": [
        "# i = 4\n",
        "# k = list(data.keys())[i]\n",
        "# x = data[k][b'data']\n",
        "# y = np.array(data[k][b'labels'])\n",
        "# model.fit(x, y, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMt8ZP6cDovo",
        "outputId": "f357d85f-9d2c-4830-d981-68da1a8e3b56"
      },
      "source": [
        "# Unseen Data\n",
        "# i = 5\n",
        "# k = list(data.keys())[i]\n",
        "k = b'testing batch 1 of 1'\n",
        "x_unseen = data[k][b'data']\n",
        "y_unseen = np.array(data[k][b'labels'])\n",
        "\n",
        "# testing on unseen data\n",
        "validation_loss, validation_accuracy = model.evaluate(x_unseen, y_unseen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8770 - accuracy: 0.7125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4S12WqTDovp",
        "outputId": "c715e8c4-13e8-4ebd-9a1c-96897a53e306"
      },
      "source": [
        "print(\"Testing loss on our unseen data: {}\".format(validation_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing loss on our unseen data: 0.877044141292572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ3Fs0r0Dovp",
        "outputId": "a7bbe6ab-50fd-4e93-813e-bf48545bef81"
      },
      "source": [
        "print(\"Testing Accuracy on our unseen data: {}\".format(validation_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy on our unseen data: 0.7124999761581421\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}